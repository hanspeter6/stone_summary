\documentclass{article}
\usepackage[utf8]{inputenc}

\title{test2}
\author{hanspeter6 }
\date{November 2019}

\begin{document}

\maketitle

\section{Introduction}


Part I All from Jones

1 Projection Pursuit

1.1 p71

Shown already:

• Signal mixtures tend to have gaussian pdf and source signals do not

• A source signal can be extracted (unmixed) by taking the inner product of a weight vector and the signal mixtures where it provides an orthogonal projection of the signal mixtures. $s=Wx$.

But, how do we find W?, the unmixing matrix (vector)

One way is exploratory projection pursuit (or just projection pursuit). This aims to find one projection at a time such that the extracted signal is as non-gaussian as possible. The name is based on the notion that the method aims to find a weight vector that provides an orthogonal projection of a set of signal mixtures such that each signal has a pdf that is as non-gaussian as possible.

Another is independent component analysis. This generally extracts M signals simultaneously from M signal mixtures. This could require the estimating of a large MxM matrix of weights. An obvious benefit of projectio pursuit is that its possible to extract fewer than M signals.

As an example: height of an individual is considered to be the sum of a genetic component and a dietary one. We assume the contributions of each is the same for all individuals (ie the ration nature/nurture is constant. That is: $h^{i}=as_{G}^{i}+bs_{D}^{i}$, representing the formation of a signal mixture h from a linear combination of the two source signals, using mixing coefficients a and b, which are the same for all individuals (ie the relative contributions of diet and genetics are the same). The CLT would suggest that the pdf of $h_{i}$ would be approximately gaussian irrespective of the pdfs of the individual contributions $s_{G}^{i}$ and $s_{D}^{i}$ .

1.2 p72

Its important to note that the converse of the CLT is not generally true. That is, it is not true that any gaussian mixture is a mixture of non-gaussian signals, although in practice this is true, ie., that a gaussian mixture consists of a mixture of non-gaussian signals. 

So, in summary: given a set of gaussian mixtures, we can find each source signal by finding the unmixing vector that extracts the most non-gaussian signal. One strategy for doing this is to define a measure of “non-gausianity” and then finding the unmixing vector that maximises this measure.

Note: We will assume that our source signals are of only one type of non-gaussian signal, ie super-gaussian. There are in fact two types: super-gaussian and sub-gaussian or platykurtotic and leptokurtotic, respectively. The first in contrast to the second has most of its values clustered around zero. (proj pursuit methods based on Kullback-Leibler divergence can extract source signals from mixtures of super- and sub-gaussian (see source: FastICA by Hyvarinen et al, 2001a) Compare with the R methods???

So, our super-gaussian signals have pdfs that are more peaky than typical of gaussian signals, implying that we will aim to find weight vectors that maximise the “peakyness” of signals.

1.3 p.73

Kurtosis is a measure of “peakyness”. So, we would try to find an unmixing vector that maximises the kurtosis of an extracted signal $y=w^{T}$ . But, first a closer look at kurtosis.

Kurtosis is defined as:





$K=\frac{\frac{1}{N}\sum_{t=1}^{N}(\bar{y}-y^{t})^{4}}{(\frac{1}{N}\sum_{t=1}^{N}(\bar{y}-y^{t})^{2})^{2}}-3$

The numerator is the important term and the constant (3) ensures that gaussian signals have zero kurtosis. Super-gaussian signals have positive kurtosis and sub-gaussian signals have negative kurtosis. The denominator is simply the variance of y, ensuring the kurtosis takes account of the signal variance.

\end{document}
